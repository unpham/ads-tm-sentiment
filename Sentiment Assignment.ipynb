{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Sentiment Assignment\n",
    "\n",
    "This notebook holds the Sentiment Assignment for Module 6 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In a previous assignment you put together Twitter data and lyrics data on two artists. In this assignment we apply sentiment analysis to those data sets. If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Blackboard. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e2e1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from string import punctuation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"/Users/lamnguyen/Dropbox/Uyen bom/USD/ADS-509/mod1/ads509_mode1_api_scrape/\"\n",
    "\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\"\n",
    "\n",
    "artist_files = {'cher':'cher_followers_data.txt',\n",
    "                'robyn':'robynkonichiwa_followers_data.txt'}\n",
    "\n",
    "\n",
    "positive_words_file = \"positive-words.txt\"\n",
    "negative_words_file = \"negative-words.txt\"\n",
    "tidy_text_file = \"tidytext_sentiments.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A Pandas data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d70801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Include Me Out\"</td>\n",
       "      <td>It is really very simple\\nJust a single pulse,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Electric\"</td>\n",
       "      <td>Electric...\\n\\nIt's electric\\nIt's a natural h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Beach 2K20\"</td>\n",
       "      <td>(So you wanna go out?\\nHow you gonna get there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Love Kills\"</td>\n",
       "      <td>If you're looking for love\\nGet a heart made o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Time Machine\"</td>\n",
       "      <td>Hey, what did I do?\\nCan't believe the fit I j...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist             title                                             lyrics\n",
       "0  robyn  \"Include Me Out\"  It is really very simple\\nJust a single pulse,...\n",
       "1  robyn        \"Electric\"  Electric...\\n\\nIt's electric\\nIt's a natural h...\n",
       "2  robyn      \"Beach 2K20\"  (So you wanna go out?\\nHow you gonna get there...\n",
       "3  robyn      \"Love Kills\"  If you're looking for love\\nGet a heart made o...\n",
       "4  robyn    \"Time Machine\"  Hey, what did I do?\\nCan't believe the fit I j..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the lyrics data\n",
    "\n",
    "#Create list of lyric data \n",
    "lyrics_data = []\n",
    "\n",
    "# Set path to the artist folders\n",
    "for artist_folder in os.listdir(data_location + lyrics_folder):\n",
    "    artist_path = os.path.join(data_location + lyrics_folder, artist_folder)\n",
    "    \n",
    "    # set path to the lyric file\n",
    "    for lyric_file in os.listdir(artist_path):\n",
    "        lyric_path = os.path.join(artist_path, lyric_file)\n",
    "            \n",
    "        # Read the lyrics from the lyric files\n",
    "        with open(lyric_path, 'r', encoding='utf8') as infile:\n",
    "            lyrics = infile.read()\n",
    "\n",
    "        # Extract the title from the lyrics\n",
    "        title = lyrics.split(\"\\n\", 1)[0].strip()\n",
    "\n",
    "        # Remove the title from the lyrics\n",
    "        lyrics = lyrics.replace(title, \"\").strip()\n",
    "\n",
    "        # Append the artist, title, and lyrics to the data list\n",
    "        lyrics_data.append([artist_folder, title, lyrics])\n",
    "\n",
    "# Create a DataFrame from the lyrics data\n",
    "lyrics_data = pd.DataFrame(lyrics_data, columns=['artist', 'title', 'lyrics'])\n",
    "lyrics_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "debcac5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>description</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hsmcnp</td>\n",
       "      <td>Country Girl</td>\n",
       "      <td>35152213</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1302</td>\n",
       "      <td>1014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>horrormomy</td>\n",
       "      <td>Jeny</td>\n",
       "      <td>742153090850164742</td>\n",
       "      <td>Earth</td>\n",
       "      <td>81</td>\n",
       "      <td>514</td>\n",
       "      <td>𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 &amp; 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anju79990584</td>\n",
       "      <td>anju</td>\n",
       "      <td>1496463006451974150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>140</td>\n",
       "      <td>163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gallionjenna</td>\n",
       "      <td>J</td>\n",
       "      <td>3366479914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>752</td>\n",
       "      <td>556</td>\n",
       "      <td>csu</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bcscomm</td>\n",
       "      <td>bcscomm</td>\n",
       "      <td>83915043</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>888</td>\n",
       "      <td>2891</td>\n",
       "      <td>Writer @Washinformer @SpelmanCollege alumna #D...</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    screen_name          name                   id        location  \\\n",
       "0        hsmcnp  Country Girl             35152213             NaN   \n",
       "1    horrormomy          Jeny   742153090850164742           Earth   \n",
       "2  anju79990584          anju  1496463006451974150             NaN   \n",
       "3  gallionjenna             J           3366479914             NaN   \n",
       "4       bcscomm       bcscomm             83915043  Washington, DC   \n",
       "\n",
       "   followers_count  friends_count  \\\n",
       "0             1302           1014   \n",
       "1               81            514   \n",
       "2               13            140   \n",
       "3              752            556   \n",
       "4              888           2891   \n",
       "\n",
       "                                         description artist  \n",
       "0                                                NaN   cher  \n",
       "1           𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 & 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜   cher  \n",
       "2          163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡   cher  \n",
       "3                                                csu   cher  \n",
       "4  Writer @Washinformer @SpelmanCollege alumna #D...   cher  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the twitter data\n",
    "twitter_data = pd.read_csv(data_location + twitter_folder + \n",
    "                           artist_files['cher'],\n",
    "                           sep=\"\\t\",\n",
    "                           quoting=3)\n",
    "\n",
    "twitter_data['artist'] = \"cher\"\n",
    "\n",
    "twitter_data_2 = pd.read_csv(data_location + twitter_folder + \n",
    "                             artist_files['robyn'],\n",
    "                             sep=\"\\t\",\n",
    "                             quoting=3)\n",
    "twitter_data_2['artist'] = \"robyn\"\n",
    "\n",
    "twitter_data = pd.concat([\n",
    "    twitter_data,twitter_data_2])\n",
    "    \n",
    "del(twitter_data_2)\n",
    "\n",
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1a3ebb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word  score\n",
      "0                 1\n",
      "1         a+      1\n",
      "2     abound      1\n",
      "3    abounds      1\n",
      "4  abundance      1\n"
     ]
    }
   ],
   "source": [
    "# Read in the positive and negative words and the\n",
    "# tidytext sentiment. Store these so that the positive\n",
    "# words are associated with a score of +1 and negative words\n",
    "# are associated with a score of -1. You can use a dataframe or a \n",
    "# dictionary for this.\n",
    "\n",
    "# Read positive words and assign score of +1\n",
    "with open(positive_words_file, 'r') as f:\n",
    "    positive_words = [line.strip() for line in f if not line.startswith(\";\")]\n",
    "    positive_df = pd.DataFrame({'word': positive_words, 'score': 1})\n",
    "    \n",
    "\n",
    "# Read negative words and assign score of -1\n",
    "with open(negative_words_file, 'r') as f:\n",
    "    negative_words = [line.strip() for line in f if not line.startswith(\";\")]\n",
    "    negative_df = pd.DataFrame({'word': negative_words, 'score': -1})\n",
    "    \n",
    "\n",
    "# Read tidytext sentiment and assign scores\n",
    "with open(tidy_text_file, 'r') as f:\n",
    "    lines = f.readlines()[1:]  # Skip the header row\n",
    "    tidytext_df = pd.DataFrame([line.strip().split('\\t') for \n",
    "                                line in lines], \n",
    "                               columns=['word', 'sentiment', 'lexicon'])\n",
    "    tidytext_df = tidytext_df[tidytext_df['sentiment'].isin(['positive', \n",
    "                                                             'negative'])]  \n",
    "    tidytext_df['score'] = tidytext_df['sentiment'].apply(lambda x: 1 if \n",
    "                                                          x == 'positive' else -1)\n",
    "\n",
    "# Combine the lexicons and drop duplicates\n",
    "combined_lexicon = pd.concat([positive_df, negative_df, \n",
    "        tidytext_df[['word', 'score']]]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Print the word scores DataFrame\n",
    "print(combined_lexicon.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Songs\n",
    "\n",
    "In this section, score the sentiment for all the songs for both artists in your data set. Score the sentiment by manually calculating the sentiment using the combined lexicons provided in this repository. \n",
    "\n",
    "After you have calculated these sentiments, answer the questions at the end of this section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ab359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some punctuation variations\n",
    "punctuation = set(string.punctuation) # speeds up comparison\n",
    "tw_punct = punctuation - {\"#\"}\n",
    "\n",
    "# Stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "# Two useful regex\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "hashtag_pattern = re.compile(r\"^#[0-9a-zA-Z]+\")\n",
    "\n",
    "# It's handy to have a full set of emojis\n",
    "all_language_emojis = set()\n",
    "\n",
    "for country in emoji.EMOJI_DATA : \n",
    "    for em in emoji.EMOJI_DATA[country] : \n",
    "        all_language_emojis.add(em)\n",
    "\n",
    "# and now our functions\n",
    "def descriptive_stats(tokens, num_tokens=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Given a list of tokens, print the number of tokens, number of unique tokens,\n",
    "    number of characters, lexical diversity, and num_tokens most common tokens.\n",
    "    Return a list of the num_tokens most common tokens.\n",
    "    \"\"\"\n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    num_characters = sum(len(token) for token in tokens)\n",
    "    lexical_diversity = num_unique_tokens / num_tokens\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Number of tokens:\", num_tokens)\n",
    "        print(\"Number of unique tokens:\", num_unique_tokens)\n",
    "        print(\"Number of characters:\", num_characters)\n",
    "        print(\"Lexical diversity:\", lexical_diversity)\n",
    "        # Print the 5 most common tokens\n",
    "        common_tokens = Counter(tokens).most_common(num_tokens)\n",
    "        print(\"The most common tokens are:\")\n",
    "        for token, count in common_tokens[:5]: #print out 5 most common token\n",
    "            print(f\"{token}: {count}\")\n",
    "    \n",
    "    #return [num_tokens, num_unique_tokens, lexical_diversity, num_characters]\n",
    "\n",
    "def contains_emoji(s):\n",
    "    s = str(s)\n",
    "    #emojis = [ch for ch in s if ch in all_language_emojis]\n",
    "    emojis = [ch for ch in s if emoji.is_emoji(ch)]\n",
    "    return len(emojis) > 0\n",
    "\n",
    "def remove_stop(tokens):\n",
    "    return [token for token in tokens if token.lower() not in sw]\n",
    "\n",
    "\n",
    "def remove_punctuation(text, punct_set=tw_punct):\n",
    "    return ''.join([c for c in text if c not in tw_punct])\n",
    "    \n",
    "\n",
    "def tokenize(text):\n",
    "#     \"\"\" Splitting on whitespace rather than the book's tokenize function. That \n",
    "#     function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def prepare(text, pipeline):\n",
    "    tokens = str(text)\n",
    "\n",
    "    for transform in pipeline:\n",
    "        tokens = transform(tokens)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0cd845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline techniques\n",
    "my_pipeline = [str.lower, remove_punctuation, tokenize, remove_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07ea3d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"None Of Dem\"</td>\n",
       "      <td>None of these boys can dance\\nNot a single one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Should Have Known\"</td>\n",
       "      <td>I should have seen it coming, I should have fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Fembot\"</td>\n",
       "      <td>I've got some news for you\\nFembots have feeli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   artist                title  \\\n",
       "8   robyn        \"None Of Dem\"   \n",
       "25  robyn  \"Should Have Known\"   \n",
       "39  robyn             \"Fembot\"   \n",
       "\n",
       "                                               lyrics  \n",
       "8   None of these boys can dance\\nNot a single one...  \n",
       "25  I should have seen it coming, I should have fu...  \n",
       "39  I've got some news for you\\nFembots have feeli...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "duplicate_rows =  lyrics_data.duplicated(subset=['artist', 'title', 'lyrics'])\n",
    "print(lyrics_data[duplicate_rows].shape)\n",
    "lyrics_data[duplicate_rows].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10d94da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "lyrics_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b5a007a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>tokens</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Include Me Out\"</td>\n",
       "      <td>It is really very simple\\nJust a single pulse,...</td>\n",
       "      <td>[really, simple, single, pulse, repeated, regu...</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Electric\"</td>\n",
       "      <td>Electric...\\n\\nIt's electric\\nIt's a natural h...</td>\n",
       "      <td>[electric, electric, natural, high, electric, ...</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Beach 2K20\"</td>\n",
       "      <td>(So you wanna go out?\\nHow you gonna get there...</td>\n",
       "      <td>[wanna, go, gonna, get, ok, call, someone, alr...</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Love Kills\"</td>\n",
       "      <td>If you're looking for love\\nGet a heart made o...</td>\n",
       "      <td>[youre, looking, love, get, heart, made, steel...</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Time Machine\"</td>\n",
       "      <td>Hey, what did I do?\\nCan't believe the fit I j...</td>\n",
       "      <td>[hey, cant, believe, fit, threw, stupid, wante...</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist             title                                             lyrics  \\\n",
       "0  robyn  \"Include Me Out\"  It is really very simple\\nJust a single pulse,...   \n",
       "1  robyn        \"Electric\"  Electric...\\n\\nIt's electric\\nIt's a natural h...   \n",
       "2  robyn      \"Beach 2K20\"  (So you wanna go out?\\nHow you gonna get there...   \n",
       "3  robyn      \"Love Kills\"  If you're looking for love\\nGet a heart made o...   \n",
       "4  robyn    \"Time Machine\"  Hey, what did I do?\\nCan't believe the fit I j...   \n",
       "\n",
       "                                              tokens  num_tokens  \n",
       "0  [really, simple, single, pulse, repeated, regu...         233  \n",
       "1  [electric, electric, natural, high, electric, ...         152  \n",
       "2  [wanna, go, gonna, get, ok, call, someone, alr...         172  \n",
       "3  [youre, looking, love, get, heart, made, steel...         244  \n",
       "4  [hey, cant, believe, fit, threw, stupid, wante...         127  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the pipeline techniques to lyrics_data\n",
    "lyrics_data[\"tokens\"] = lyrics_data[\"lyrics\"].apply(prepare, \n",
    "                                                    pipeline=my_pipeline)\n",
    "lyrics_data[\"num_tokens\"] = lyrics_data[\"tokens\"].map(len)\n",
    "\n",
    "lyrics_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b762cb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>tokens</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Include Me Out\"</td>\n",
       "      <td>It is really very simple\\nJust a single pulse,...</td>\n",
       "      <td>[really, simple, single, pulse, repeated, regu...</td>\n",
       "      <td>233</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Electric\"</td>\n",
       "      <td>Electric...\\n\\nIt's electric\\nIt's a natural h...</td>\n",
       "      <td>[electric, electric, natural, high, electric, ...</td>\n",
       "      <td>152</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Beach 2K20\"</td>\n",
       "      <td>(So you wanna go out?\\nHow you gonna get there...</td>\n",
       "      <td>[wanna, go, gonna, get, ok, call, someone, alr...</td>\n",
       "      <td>172</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Love Kills\"</td>\n",
       "      <td>If you're looking for love\\nGet a heart made o...</td>\n",
       "      <td>[youre, looking, love, get, heart, made, steel...</td>\n",
       "      <td>244</td>\n",
       "      <td>-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Time Machine\"</td>\n",
       "      <td>Hey, what did I do?\\nCan't believe the fit I j...</td>\n",
       "      <td>[hey, cant, believe, fit, threw, stupid, wante...</td>\n",
       "      <td>127</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist             title                                             lyrics  \\\n",
       "0  robyn  \"Include Me Out\"  It is really very simple\\nJust a single pulse,...   \n",
       "1  robyn        \"Electric\"  Electric...\\n\\nIt's electric\\nIt's a natural h...   \n",
       "2  robyn      \"Beach 2K20\"  (So you wanna go out?\\nHow you gonna get there...   \n",
       "3  robyn      \"Love Kills\"  If you're looking for love\\nGet a heart made o...   \n",
       "4  robyn    \"Time Machine\"  Hey, what did I do?\\nCan't believe the fit I j...   \n",
       "\n",
       "                                              tokens  num_tokens  \\\n",
       "0  [really, simple, single, pulse, repeated, regu...         233   \n",
       "1  [electric, electric, natural, high, electric, ...         152   \n",
       "2  [wanna, go, gonna, get, ok, call, someone, alr...         172   \n",
       "3  [youre, looking, love, get, heart, made, steel...         244   \n",
       "4  [hey, cant, believe, fit, threw, stupid, wante...         127   \n",
       "\n",
       "   sentiment_score  \n",
       "0               11  \n",
       "1               24  \n",
       "2               26  \n",
       "3              -24  \n",
       "4               -8  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to calculate sentiment score for a given song lyrics\n",
    "def calculate_sentiment_score(lyrics):\n",
    "    tokens = prepare(lyrics, pipeline=my_pipeline)\n",
    "    sentiment_score = 0\n",
    "    for token in tokens:\n",
    "        # Lookup the word in the combined_lexicon DataFrame\n",
    "        word_score = combined_lexicon.loc[combined_lexicon['word'] == token, \n",
    "                                          'score'].values\n",
    "        if len(word_score) > 0:\n",
    "             # Add the word's score to the sentiment score\n",
    "            sentiment_score += word_score[0] \n",
    "    return sentiment_score\n",
    "\n",
    "# Apply sentiment scoring to the lyrics_data DataFrame\n",
    "lyrics_data['sentiment_score'] = lyrics_data['lyrics'].apply(calculate_sentiment_score)\n",
    "\n",
    "# Print the lyrics_data DataFrame with sentiment scores\n",
    "lyrics_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae279d39",
   "metadata": {},
   "source": [
    "## Average sentiment per song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99065e04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sentiment per Song:\n",
      "artist\n",
      "cher     5.699367\n",
      "robyn    8.652632\n",
      "Name: sentiment_score, dtype: float64\n",
      "\n",
      "Artist with Higher Average Sentiment:\n",
      "robyn\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average sentiment per song for each artist\n",
    "average_sentiment = lyrics_data.groupby('artist')['sentiment_score'].mean()\n",
    "\n",
    "# Determine the artist with the higher average sentiment\n",
    "artist_with_higher_sentiment = average_sentiment.idxmax()\n",
    "\n",
    "# Print the average sentiment per song for each artist\n",
    "print(\"Average Sentiment per Song:\")\n",
    "print(average_sentiment)\n",
    "\n",
    "# Print the artist with the higher average sentiment\n",
    "print(\"\\nArtist with Higher Average Sentiment:\")\n",
    "print(artist_with_higher_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481cafdd",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "Q: Overall, which artist has the higher average sentiment per song? \n",
    "\n",
    "A: Robyn has the higher average sentiment per song\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42312836",
   "metadata": {},
   "source": [
    "## Highest and lowest sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7572bc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Songs with Highest Sentiment:\n",
      "   artist                   title  sentiment_score\n",
      "21  robyn          \"Love Is Free\"              125\n",
      "50  robyn  \"We Dance To The Beat\"               66\n",
      "49  robyn     \"Between The Lines\"               52\n",
      "------------------------------------------------------------\n",
      "Songs with Lowest Sentiment:\n",
      "   artist                               title  sentiment_score\n",
      "53  robyn  \"Don't Fucking Tell Me What To Do\"              -90\n",
      "16  robyn                   \"Criminal Intent\"              -51\n",
      "3   robyn                        \"Love Kills\"              -24\n"
     ]
    }
   ],
   "source": [
    "first_artist_lyrics = lyrics_data[lyrics_data['artist'] == 'robyn']\n",
    "# Sort the songs by sentiment score in descending order to get the highest sentiment songs\n",
    "highest_sentiment_songs = first_artist_lyrics.sort_values('sentiment_score',\n",
    "                                                          ascending=False).head(3)\n",
    "\n",
    "# Sort the songs by sentiment score in ascending order to get the lowest sentiment songs\n",
    "lowest_sentiment_songs = first_artist_lyrics.sort_values('sentiment_score').head(3)\n",
    "\n",
    "print(\"Songs with Highest Sentiment:\")\n",
    "print(highest_sentiment_songs[['artist', 'title', 'sentiment_score']])\n",
    "print('------------------------------------------------------------')\n",
    "print(\"Songs with Lowest Sentiment:\")\n",
    "print(lowest_sentiment_songs[['artist', 'title', 'sentiment_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f741d0",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "Q: For your first artist, what are the three songs that have the highest and lowest sentiments? Print the lyrics of those songs to the screen. What do you think is driving the sentiment score? \n",
    "\n",
    "A: The sentiment scores of songs show how positive or negative they are. Factors like lyrics, music, tone, and theme contribute to these scores. Robyn's songs with high sentiment scores, like \"Love Is Free\" and \"We Dance To The Beat,\" have uplifting and joyful themes. On the other hand, songs with low sentiment scores, like \"Don't Fucking Tell Me What To Do,\" express anger or frustration. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f147b262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Songs with Highest Sentiment:\n",
      "    artist                     title  sentiment_score\n",
      "250   cher  \"Love And Understanding\"               58\n",
      "207   cher              \"Perfection\"               49\n",
      "338   cher        \"I Found You Love\"               47\n",
      "--------------------------------------------------------------\n",
      "Songs with Lowest Sentiment:\n",
      "    artist                               title  sentiment_score\n",
      "145   cher                         \"Bang-Bang\"              -71\n",
      "299   cher  \"Bang Bang (My Baby Shot Me Down)\"              -33\n",
      "262   cher                        \"Outrageous\"              -30\n"
     ]
    }
   ],
   "source": [
    "second_artist_lyrics = lyrics_data[lyrics_data['artist'] == 'cher']\n",
    "# Sort the songs by sentiment score in descending order to get the highest sentiment songs\n",
    "highest_sentiment_songs = second_artist_lyrics.sort_values('sentiment_score', \n",
    "                                                           ascending=False).head(3)\n",
    "\n",
    "# Sort the songs by sentiment score in ascending order to get the lowest sentiment songs\n",
    "lowest_sentiment_songs = second_artist_lyrics.sort_values('sentiment_score').head(3)\n",
    "\n",
    "print(\"Songs with Highest Sentiment:\")\n",
    "print(highest_sentiment_songs[['artist', 'title', 'sentiment_score']])\n",
    "print('--------------------------------------------------------------')\n",
    "print(\"Songs with Lowest Sentiment:\")\n",
    "print(lowest_sentiment_songs[['artist', 'title', 'sentiment_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6844a0",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "Q: For your second artist, what are the three songs that have the highest and lowest sentiments? Print the lyrics of those songs to the screen. What do you think is driving the sentiment score? \n",
    "\n",
    "A: The sentiment scores reveal the emotional intensity of songs. Among Cher's songs, \"Love And Understanding,\" \"Perfection,\" and \"I Found You Love\" have the highest sentiment scores, indicating they evoke strong positive emotions. On the other hand, \"Bang-Bang,\" \"Bang Bang (My Baby Shot Me Down),\" and \"Outrageous\" have the lowest sentiment scores, suggesting a more negative tone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c354d1e9",
   "metadata": {},
   "source": [
    "### Q: Plot the distributions of the sentiment scores for both artists. You can use `seaborn` to plot densities or plot histograms in matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d666b1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "robyn_scores = first_artist_lyrics['sentiment_score']\n",
    "cher_scores = second_artist_lyrics['sentiment_score']\n",
    "# Plot the distributions\n",
    "plt.hist(robyn_scores, bins='auto', density=True, alpha=0.5, \n",
    "         label='Robyn')\n",
    "plt.hist(cher_scores, bins='auto', density=True, alpha=0.5, \n",
    "         label='Cher')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Sentiment Scores')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe644d",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Twitter Descriptions\n",
    "\n",
    "In this section, define two sets of emojis you designate as positive and negative. Make sure to have at least 10 emojis per set. You can learn about the most popular emojis on Twitter at [the emojitracker](https://emojitracker.com/). \n",
    "\n",
    "Associate your positive emojis with a score of +1, negative with -1. Score the average sentiment of your two artists based on the Twitter descriptions of their followers. The average sentiment can just be the total score divided by number of followers. You do not need to calculate sentiment on non-emoji content for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fa66b9",
   "metadata": {},
   "source": [
    "\n",
    "### Q: Which positive emoji is the most popular for each artist? Which negative emoji? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f40f6ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentiment score for cher: 0.9517043983758477\n",
      "Average sentiment score for robyn: 0.9724534895672731\n"
     ]
    }
   ],
   "source": [
    "#create positive and negative sets with assigned scores\n",
    "positive_emojis = {'😄': 1, '👍': 1, '❤️': 1, '😊': 1, '🌟': 1, '😎': 1, '🎉': 1, \n",
    "                   '🥰': 1, '👏': 1, '😍': 1, '🌈': 1, '✨': 1, '💖': 1}\n",
    "negative_emojis = {'😔': -1, '👎': -1, '😞': -1, '😢': -1, '😒': -1, '🙁': -1, '😑': -1, \n",
    "                   '😠': -1, '😡': -1, '😣': -1, '😕': -1, '💔': -1, '😫': -1 }\n",
    "\n",
    "#Score the average sentiment of your two artists based on the Twitter descriptions of their followers\n",
    "artist_sentiment = {}\n",
    "top_positive_emojis = {}\n",
    "top_negative_emojis = {}\n",
    "\n",
    "for artist, descriptions in twitter_data.groupby('artist')['description']:\n",
    "    total_score = 0\n",
    "    total_count = 0\n",
    "    positive_counts = {}\n",
    "    negative_counts = {}\n",
    "\n",
    "    for description in descriptions:\n",
    "        if isinstance(description, str): \n",
    "            for emoji in positive_emojis:\n",
    "                if emoji in description:\n",
    "                    positive_counts[emoji] = positive_counts.get(emoji, 0) + 1\n",
    "                    total_score += positive_emojis[emoji]\n",
    "                    total_count += 1\n",
    "\n",
    "            for emoji in negative_emojis:\n",
    "                if emoji in description:\n",
    "                    negative_counts[emoji] = negative_counts.get(emoji, 0) + 1\n",
    "                    total_score += negative_emojis[emoji]\n",
    "                    total_count += 1\n",
    "                    \n",
    "    top_positive_emojis[artist] = sorted(positive_counts, \n",
    "                                         key=positive_counts.get, \n",
    "                                         reverse=True)[:3]\n",
    "    top_negative_emojis[artist] = sorted(negative_counts, \n",
    "                                         key=negative_counts.get, \n",
    "                                         reverse=True)[:3]\n",
    "\n",
    "    average_sentiment = total_score / total_count if total_count > 0 else 0\n",
    "    artist_sentiment[artist] = average_sentiment\n",
    "    print(f\"Average sentiment score for {artist}: {average_sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265127c",
   "metadata": {},
   "source": [
    "### Q: Which positive emoji is the most popular for each artist? Which negative emoji? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce50996b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 most popular positive emojis:\n",
      "cher: ['🌈', '❤️', '✨']\n",
      "robyn: ['🌈', '❤️', '✨']\n",
      "\n",
      "Top 3 most popular negative emojis:\n",
      "cher: ['💔', '😔', '😢']\n",
      "robyn: ['💔', '😒', '😔']\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 3 most popular positive emojis:\")\n",
    "for artist, emojis in top_positive_emojis.items():\n",
    "    print(f\"{artist}: {emojis}\")\n",
    "print()\n",
    "print(\"Top 3 most popular negative emojis:\")\n",
    "for artist, emojis in top_negative_emojis.items():\n",
    "    print(f\"{artist}: {emojis}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b5c008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
